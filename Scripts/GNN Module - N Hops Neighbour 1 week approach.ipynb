{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19aa2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6011a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28887dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61671cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df48163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_path = '../Pickle_Files/'\n",
    "csv_path = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7828e7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df = pd.read_csv(csv_path+\"User_profile.csv\")\n",
    "sample_data = pickle_loader(Pickle_path+\"URL_base_data_train_test_POC.pkl\")\n",
    "Follower_cnt_dict = pickle_loader(Pickle_path+\"Follower_cnt_dict.pkl\")\n",
    "similarity_dict = pickle_loader(Pickle_path+\"node_similarity_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f49b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assume your DataFrame is named df\n",
    "tw_usr_screen_dict = User_level_feat_df.reset_index().set_index('tw_usr_screen')['index'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f2ef95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eca8e46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3b96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Follower_cnt_dict_idx = {}\n",
    "\n",
    "for key, val in Follower_cnt_dict.items():\n",
    "    Follower_cnt_dict_idx[tw_usr_screen_dict[key]] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3603cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict_idx = {}\n",
    "\n",
    "for user,adj in similarity_dict.items():\n",
    "    inner_dict = {}\n",
    "    for key,val in adj.items():\n",
    "        inner_dict[tw_usr_screen_dict[key]] = val\n",
    "    similarity_dict_idx[tw_usr_screen_dict[user]] = inner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3e01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fe967ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6197fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of nodes (users)\n",
    "num_nodes = len(similarity_dict_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98ca1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the edge list and edge weights\n",
    "edge_list = []\n",
    "# edge_weights = []\n",
    "for node1, neighbors in similarity_dict_idx.items():\n",
    "    for node2, weight in neighbors.items():\n",
    "        if node1!=node2:\n",
    "            edge_list.append((node1, node2,weight))\n",
    "#         edge_weights.append(weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4d7c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "edge_list_tmp = sorted(edge_list, key=lambda x: x[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e637568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loops = []\n",
    "for pair in edge_list_tmp:\n",
    "    if pair[0]==pair[1]:\n",
    "        loops.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8140f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = []\n",
    "for pairs in edge_list:\n",
    "    edge_weights.append(pairs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccc7c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = [w[:2] for w in edge_list_tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff264c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = {k: v for k, v in sorted(Follower_cnt_dict_idx.items())}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aee9c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.zeros((num_nodes, 2))\n",
    "\n",
    "\n",
    "num_node_features = 2\n",
    "\n",
    "for node, feature_vector in sorted_dict.items():\n",
    "    try:\n",
    "        features[node] = torch.tensor([int(feature_vector),int(feature_vector)])\n",
    "    except:\n",
    "        print(node)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c0a58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Base_graph = Data(x=features, edge_index=torch.tensor(edge_list).t().contiguous(), edge_attr=torch.tensor(edge_weights), y = torch.tensor(10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d61d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[98896, 2], edge_index=[2, 869989], edge_attr=[869989], y=10000)\n",
      "=============================================================\n",
      "Number of nodes: 98896\n",
      "Number of edges: 869989\n",
      "Average node degree: 8.80\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = Base_graph  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdffbdc1",
   "metadata": {},
   "source": [
    "### Load URL data to generate train,test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef614174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "sample_data['usr_screen_60'] = sample_data.apply(lambda row: tuple(set((row['rtw_usr_screen_list_60']+ row['tw_usr_screen_list_60']))), axis=1)\n",
    "\n",
    "\n",
    "sample_data['usr_screen_all'] = sample_data.apply(lambda row: tuple(set((row['rtw_usr_screen_list_all']+ row['tw_usr_screen_list_all']))), axis=1)\n",
    "\n",
    "\n",
    "def extract_user_ids(screen_names, mapping_dict):\n",
    "    user_ids = []\n",
    "    for screen_name in screen_names:\n",
    "        if screen_name in mapping_dict:\n",
    "            user_ids.append(mapping_dict[screen_name])\n",
    "    return tuple(user_ids)\n",
    "\n",
    "\n",
    "sample_data['train_user_ids'] = sample_data['usr_screen_60'].apply(lambda x: extract_user_ids( x, tw_usr_screen_dict))\n",
    "sample_data['test_user_ids'] = sample_data['usr_screen_all'].apply(lambda x: extract_user_ids(x, tw_usr_screen_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1534b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def node_feature_creation(sorted_dict,Is_url_Tweet_lst):\n",
    "    features = torch.zeros((num_nodes, 2))\n",
    "#     print(Is_url_Tweet_lst)\n",
    "#     print(type(Is_url_Tweet_lst))\n",
    "    num_node_features = 2\n",
    "    cnt = 0\n",
    "    for node, feature_vector in sorted_dict.items():\n",
    "        try:\n",
    "            if node in Is_url_Tweet_lst:\n",
    "                features[node] = torch.tensor([int(feature_vector),1])\n",
    "                cnt+=1\n",
    "            else:\n",
    "                features[node] = torch.tensor([int(feature_vector),0])\n",
    "        except:\n",
    "            print(node)\n",
    "            break\n",
    "#     print(cnt)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089fbdb",
   "metadata": {},
   "source": [
    "### Train test Graph Generation Module\n",
    "\n",
    "SO we have static edge list and edge weight list.\n",
    "\n",
    "we create dynamic node features\n",
    "    1. Follower COunt\n",
    "    2. Binary flag whether a node tweeted a URL or not\n",
    "\n",
    "Target Y is : Retweet Count\n",
    "\n",
    "It will vary from train to test as we take only 60% data per URl in training and remaining in test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33911ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Graph(train_nodes,y_val):\n",
    "\n",
    "\n",
    "    Level_1_Neighbours = []\n",
    "\n",
    "    Level_2_Neighbours = []\n",
    "\n",
    "    Node_Feature_list = set()\n",
    "\n",
    "    Node_set = set()\n",
    "\n",
    "\n",
    "    for root_node in train_nodes:\n",
    "        if root_node in similarity_dict_idx.keys():\n",
    "            Node_set.add(root_node)\n",
    "\n",
    "            follower_cnt = int(Follower_cnt_dict_idx.get(root_node,0))\n",
    "\n",
    "            Node_Feature_list.add((root_node,follower_cnt,1))   ### add 2 features - 1 is follower cnt, 2 is binary flag that they tweeted url\n",
    "\n",
    "            neighbours = similarity_dict_idx.get(root_node)\n",
    "            for neighbour, weight in neighbours.items():\n",
    "                if neighbour!=root_node:\n",
    "                    Node_set.add(neighbour)\n",
    "                    Level_1_Neighbours.append((root_node,neighbour,weight))\n",
    "                    follower_cnt = int(Follower_cnt_dict_idx.get(neighbour,0))\n",
    "                    if neighbour not in train_nodes:\n",
    "                        Node_Feature_list.add((neighbour,follower_cnt,0))   ### add 2 features - 1 is follower cnt, 2 is binary flag that they tweeted url\n",
    "\n",
    "                else:\n",
    "                    continue\n",
    "                if neighbour in  similarity_dict_idx.keys():\n",
    "                    neighbour_l2 = similarity_dict_idx.get(neighbour)\n",
    "                    for neighbour2, weight2 in neighbour_l2.items():\n",
    "                        if neighbour!=neighbour2:\n",
    "                            Node_set.add(neighbour2)\n",
    "                            Level_2_Neighbours.append((neighbour,neighbour2,weight2))\n",
    "                            follower_cnt = int(Follower_cnt_dict_idx.get(neighbour2,0))\n",
    "                            if neighbour2 not in train_nodes:\n",
    "                                Node_Feature_list.add((neighbour2,follower_cnt,0))   ### add 2 features - 1 is follower cnt, 2 is binary flag that they tweeted url\n",
    "\n",
    "                        else:\n",
    "                            continue\n",
    "                \n",
    "                \n",
    "    #### IN aboe code we first identify immediate neighbours of nodes who tweeted a URL, Level_1_Neighbours\n",
    "    #### then we first second level enighbours - Friend of Friend - stored in  Level_2_Neighbours          \n",
    "    \n",
    "    edge_list_1 = Level_1_Neighbours+Level_2_Neighbours  \n",
    "    node_index_idx = {}\n",
    "\n",
    "    for index,node in enumerate(Node_set):\n",
    "    #     print(index,node)\n",
    "        node_index_idx[node] = index\n",
    "\n",
    "    edge_list_dedup_1 = list(set(edge_list_1))\n",
    "\n",
    "    edge_list_dedup_2 = [(node_index_idx[w[0]],node_index_idx[w[1]],w[2]) for w in edge_list_dedup_1]\n",
    "    edge_list_dedup = sorted(edge_list_dedup_2, key=lambda x: x[0])\n",
    "    edge_weights = []\n",
    "    for pairs in edge_list_dedup:\n",
    "        edge_weights.append(pairs[2])\n",
    "    edge_list = [w[:2] for w in edge_list_dedup]\n",
    "    Node_Feature_list_tmp = []\n",
    "\n",
    "    for node in Node_Feature_list:\n",
    "        new_key = node_index_idx[node[0]]\n",
    "        Node_Feature_list_tmp.append((new_key,node[1],node[2]))\n",
    "    Node_Feature_list_sorted = sorted(Node_Feature_list_tmp, key=lambda x: x[0])\n",
    "    features = torch.zeros((len(Node_Feature_list_sorted), 2))\n",
    "\n",
    "\n",
    "    num_node_features = 2\n",
    "\n",
    "    for node, feature_vector in enumerate(Node_Feature_list_sorted):\n",
    "        try:\n",
    "            features[node] = torch.tensor([int(feature_vector[1]),int(feature_vector[2])])\n",
    "        except:\n",
    "            print(node)\n",
    "            break\n",
    "    Num_nodes = len(features)\n",
    "    graph_trn = Data(x=features, edge_index=torch.tensor(edge_list).T, edge_attr=torch.tensor(edge_weights), y = torch.tensor(y_val))\n",
    "    return graph_trn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba80854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b6da9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def exract_rand_60pc(my_list):\n",
    "    my_list = list(my_list)\n",
    "\n",
    "#     my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    n = len(my_list)\n",
    "    k = int(0.6 * n)  # select 60% of elements\n",
    "\n",
    "    random.shuffle(my_list)  # shuffle the list randomly\n",
    "\n",
    "    return my_list[:k] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dcbcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_train = []\n",
    "Dataset_test = []\n",
    "\n",
    "for index, row in sample_data.iterrows():\n",
    "#     uniform_var = np.random.rand() \n",
    "#     if uniform_var>0.75:\n",
    "        feat_list_train  = exract_rand_60pc(row[\"train_user_ids\"])\n",
    "        y_val_train = int(row['retweet_cnt_sum_60'])\n",
    "        y_val_test = int(row['retweet_cnt_sum_all'])\n",
    "        \n",
    "#         graph_trn = Data(x=feat_list_train, edge_index=torch.tensor(edge_list).T, edge_attr=torch.tensor(edge_weights), y =y_val_train )\n",
    "        graph_trn = Generate_Graph(feat_list_train,np.log10(1+y_val_train))\n",
    "        Dataset_train.append(graph_trn)\n",
    "#     uniform_var = np.random.rand() \n",
    "\n",
    "#     if uniform_var>0.75:\n",
    "        feat_list_test  = exract_rand_60pc(row[\"test_user_ids\"])\n",
    "        y_val_test = int(row['retweet_cnt_sum_all'])   ### lets try - np.log(10)\n",
    "#         graph_test = Data(x=feat_list_test, edge_index=torch.tensor(edge_list).T, edge_attr=torch.tensor(edge_weights), y =y_val_test )\n",
    "        graph_test = Generate_Graph(feat_list_test,np.log10(1+y_val_test))\n",
    "\n",
    "        Dataset_test.append(graph_test)\n",
    "#         break\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0664fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21d90aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08dcd1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[4205, 2], edge_index=[2, 7896], edge_attr=[7896], y=[16], batch=[4205], ptr=[17])\n",
      "\n",
      "Step 2:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[5022, 2], edge_index=[2, 9846], edge_attr=[9846], y=[16], batch=[5022], ptr=[17])\n",
      "\n",
      "Step 3:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[4537, 2], edge_index=[2, 8607], edge_attr=[8607], y=[16], batch=[4537], ptr=[17])\n",
      "\n",
      "Step 4:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[3828, 2], edge_index=[2, 8442], edge_attr=[8442], y=[16], batch=[3828], ptr=[17])\n",
      "\n",
      "Step 5:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[5037, 2], edge_index=[2, 9532], edge_attr=[9532], y=[16], batch=[5037], ptr=[17])\n",
      "\n",
      "Step 6:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[4128, 2], edge_index=[2, 8602], edge_attr=[8602], y=[16], batch=[4128], ptr=[17])\n",
      "\n",
      "Step 7:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[3590, 2], edge_index=[2, 6830], edge_attr=[6830], y=[16], batch=[3590], ptr=[17])\n",
      "\n",
      "Step 8:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[6449, 2], edge_index=[2, 12725], edge_attr=[12725], y=[16], batch=[6449], ptr=[17])\n",
      "\n",
      "Step 9:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[3145, 2], edge_index=[2, 6025], edge_attr=[6025], y=[16], batch=[3145], ptr=[17])\n",
      "\n",
      "Step 10:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[5993, 2], edge_index=[2, 11167], edge_attr=[11167], y=[16], batch=[5993], ptr=[17])\n",
      "\n",
      "Step 11:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[5569, 2], edge_index=[2, 10448], edge_attr=[10448], y=[16], batch=[5569], ptr=[17])\n",
      "\n",
      "Step 12:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[3293, 2], edge_index=[2, 6100], edge_attr=[6100], y=[16], batch=[3293], ptr=[17])\n",
      "\n",
      "Step 13:\n",
      "=======\n",
      "Number of graphs in the current batch: 16\n",
      "DataBatch(x=[3812, 2], edge_index=[2, 7098], edge_attr=[7098], y=[16], batch=[3812], ptr=[17])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(Dataset_train, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(Dataset_test, batch_size=16, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c21449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21af0c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(2, 16)\n",
      "  (conv2): GraphConv(16, 16)\n",
      "  (conv3): GraphConv(16, 16)\n",
      "  (conv4): GraphConv(16, 16)\n",
      "  (conv5): GraphConv(16, 16)\n",
      "  (lin): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,GraphConv\n",
    "from torch_geometric.nn import global_mean_pool,global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(23311)\n",
    "        self.conv1 = GCNConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GraphConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv5(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d348c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8190f073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model_0point11_logpred.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff4c1e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(2, 256)\n",
      "  (conv2): GraphConv(256, 256)\n",
      "  (conv3): GraphConv(256, 256)\n",
      "  (conv4): GraphConv(256, 256)\n",
      "  (conv5): GraphConv(256, 256)\n",
      "  (lin): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8a1f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train MSE: 0.1410, Test MSE: 0.1251\n",
      "Epoch: 002, Train MSE: 0.1383, Test MSE: 0.1169\n",
      "Epoch: 003, Train MSE: 0.1375, Test MSE: 0.1165\n",
      "Epoch: 004, Train MSE: 0.1376, Test MSE: 0.1215\n",
      "Epoch: 005, Train MSE: 0.1367, Test MSE: 0.1157\n",
      "Epoch: 006, Train MSE: 0.1360, Test MSE: 0.1187\n",
      "Epoch: 007, Train MSE: 0.1360, Test MSE: 0.1151\n",
      "Epoch: 008, Train MSE: 0.1356, Test MSE: 0.1149\n",
      "Epoch: 009, Train MSE: 0.1345, Test MSE: 0.1169\n",
      "Epoch: 010, Train MSE: 0.1353, Test MSE: 0.1139\n",
      "Epoch: 011, Train MSE: 0.1340, Test MSE: 0.1158\n",
      "Epoch: 012, Train MSE: 0.1350, Test MSE: 0.1220\n",
      "Epoch: 013, Train MSE: 0.1340, Test MSE: 0.1141\n",
      "Epoch: 014, Train MSE: 0.1332, Test MSE: 0.1162\n",
      "Epoch: 015, Train MSE: 0.1341, Test MSE: 0.1134\n",
      "Epoch: 016, Train MSE: 0.1334, Test MSE: 0.1189\n",
      "Epoch: 017, Train MSE: 0.1333, Test MSE: 0.1143\n",
      "Epoch: 018, Train MSE: 0.1330, Test MSE: 0.1145\n",
      "Epoch: 019, Train MSE: 0.1330, Test MSE: 0.1144\n",
      "Epoch: 020, Train MSE: 0.1331, Test MSE: 0.1188\n",
      "Epoch: 021, Train MSE: 0.1326, Test MSE: 0.1155\n",
      "Epoch: 022, Train MSE: 0.1330, Test MSE: 0.1192\n",
      "Epoch: 023, Train MSE: 0.1347, Test MSE: 0.1124\n",
      "Epoch: 024, Train MSE: 0.1324, Test MSE: 0.1166\n",
      "Epoch: 025, Train MSE: 0.1323, Test MSE: 0.1153\n",
      "Epoch: 026, Train MSE: 0.1323, Test MSE: 0.1165\n",
      "Epoch: 027, Train MSE: 0.1324, Test MSE: 0.1142\n",
      "Epoch: 028, Train MSE: 0.1324, Test MSE: 0.1180\n",
      "Epoch: 029, Train MSE: 0.1321, Test MSE: 0.1162\n",
      "Epoch: 030, Train MSE: 0.1336, Test MSE: 0.1125\n",
      "Epoch: 031, Train MSE: 0.1321, Test MSE: 0.1150\n",
      "Epoch: 032, Train MSE: 0.1322, Test MSE: 0.1140\n",
      "Epoch: 033, Train MSE: 0.1326, Test MSE: 0.1132\n",
      "Epoch: 034, Train MSE: 0.1319, Test MSE: 0.1152\n",
      "Epoch: 035, Train MSE: 0.1326, Test MSE: 0.1131\n",
      "Epoch: 036, Train MSE: 0.1319, Test MSE: 0.1168\n",
      "Epoch: 037, Train MSE: 0.1319, Test MSE: 0.1146\n",
      "Epoch: 038, Train MSE: 0.1319, Test MSE: 0.1146\n",
      "Epoch: 039, Train MSE: 0.1317, Test MSE: 0.1160\n",
      "Epoch: 040, Train MSE: 0.1320, Test MSE: 0.1137\n",
      "Epoch: 041, Train MSE: 0.1317, Test MSE: 0.1161\n",
      "Epoch: 042, Train MSE: 0.1321, Test MSE: 0.1135\n",
      "Epoch: 043, Train MSE: 0.1318, Test MSE: 0.1175\n",
      "Epoch: 044, Train MSE: 0.1319, Test MSE: 0.1137\n",
      "Epoch: 045, Train MSE: 0.1327, Test MSE: 0.1207\n",
      "Epoch: 046, Train MSE: 0.1322, Test MSE: 0.1129\n",
      "Epoch: 047, Train MSE: 0.1317, Test MSE: 0.1175\n",
      "Epoch: 048, Train MSE: 0.1319, Test MSE: 0.1133\n",
      "Epoch: 049, Train MSE: 0.1314, Test MSE: 0.1147\n",
      "Epoch: 050, Train MSE: 0.1319, Test MSE: 0.1131\n",
      "Epoch: 051, Train MSE: 0.1318, Test MSE: 0.1185\n",
      "Epoch: 052, Train MSE: 0.1317, Test MSE: 0.1133\n",
      "Epoch: 053, Train MSE: 0.1331, Test MSE: 0.1119\n",
      "Epoch: 054, Train MSE: 0.1338, Test MSE: 0.1243\n",
      "Epoch: 055, Train MSE: 0.1311, Test MSE: 0.1154\n",
      "Epoch: 056, Train MSE: 0.1311, Test MSE: 0.1147\n",
      "Epoch: 057, Train MSE: 0.1314, Test MSE: 0.1133\n",
      "Epoch: 058, Train MSE: 0.1311, Test MSE: 0.1146\n",
      "Epoch: 059, Train MSE: 0.1312, Test MSE: 0.1172\n",
      "Epoch: 060, Train MSE: 0.1310, Test MSE: 0.1160\n",
      "Epoch: 061, Train MSE: 0.1310, Test MSE: 0.1142\n",
      "Epoch: 062, Train MSE: 0.1309, Test MSE: 0.1158\n",
      "Epoch: 063, Train MSE: 0.1320, Test MSE: 0.1123\n",
      "Epoch: 064, Train MSE: 0.1312, Test MSE: 0.1180\n",
      "Epoch: 065, Train MSE: 0.1314, Test MSE: 0.1129\n",
      "Epoch: 066, Train MSE: 0.1309, Test MSE: 0.1170\n",
      "Epoch: 067, Train MSE: 0.1307, Test MSE: 0.1158\n",
      "Epoch: 068, Train MSE: 0.1312, Test MSE: 0.1130\n",
      "Epoch: 069, Train MSE: 0.1320, Test MSE: 0.1212\n",
      "Epoch: 070, Train MSE: 0.1318, Test MSE: 0.1122\n",
      "Epoch: 071, Train MSE: 0.1313, Test MSE: 0.1192\n",
      "Epoch: 072, Train MSE: 0.1306, Test MSE: 0.1148\n",
      "Epoch: 073, Train MSE: 0.1309, Test MSE: 0.1133\n",
      "Epoch: 074, Train MSE: 0.1306, Test MSE: 0.1141\n",
      "Epoch: 075, Train MSE: 0.1306, Test MSE: 0.1137\n",
      "Epoch: 076, Train MSE: 0.1304, Test MSE: 0.1145\n",
      "Epoch: 077, Train MSE: 0.1305, Test MSE: 0.1138\n",
      "Epoch: 078, Train MSE: 0.1311, Test MSE: 0.1194\n",
      "Epoch: 079, Train MSE: 0.1314, Test MSE: 0.1121\n",
      "Epoch: 080, Train MSE: 0.1303, Test MSE: 0.1145\n",
      "Epoch: 081, Train MSE: 0.1303, Test MSE: 0.1141\n",
      "Epoch: 082, Train MSE: 0.1310, Test MSE: 0.1193\n",
      "Epoch: 083, Train MSE: 0.1302, Test MSE: 0.1151\n",
      "Epoch: 084, Train MSE: 0.1302, Test MSE: 0.1157\n",
      "Epoch: 085, Train MSE: 0.1303, Test MSE: 0.1135\n",
      "Epoch: 086, Train MSE: 0.1310, Test MSE: 0.1194\n",
      "Epoch: 087, Train MSE: 0.1323, Test MSE: 0.1114\n",
      "Epoch: 088, Train MSE: 0.1303, Test MSE: 0.1175\n",
      "Epoch: 089, Train MSE: 0.1305, Test MSE: 0.1127\n",
      "Epoch: 090, Train MSE: 0.1299, Test MSE: 0.1147\n",
      "Epoch: 091, Train MSE: 0.1299, Test MSE: 0.1150\n",
      "Epoch: 092, Train MSE: 0.1300, Test MSE: 0.1163\n",
      "Epoch: 093, Train MSE: 0.1311, Test MSE: 0.1118\n",
      "Epoch: 094, Train MSE: 0.1306, Test MSE: 0.1192\n",
      "Epoch: 095, Train MSE: 0.1298, Test MSE: 0.1138\n",
      "Epoch: 096, Train MSE: 0.1297, Test MSE: 0.1151\n",
      "Epoch: 097, Train MSE: 0.1301, Test MSE: 0.1179\n",
      "Epoch: 098, Train MSE: 0.1300, Test MSE: 0.1128\n",
      "Epoch: 099, Train MSE: 0.1311, Test MSE: 0.1211\n",
      "Epoch: 100, Train MSE: 0.1307, Test MSE: 0.1119\n",
      "Epoch: 101, Train MSE: 0.1299, Test MSE: 0.1176\n",
      "Epoch: 102, Train MSE: 0.1299, Test MSE: 0.1129\n",
      "Epoch: 103, Train MSE: 0.1295, Test MSE: 0.1149\n",
      "Epoch: 104, Train MSE: 0.1295, Test MSE: 0.1160\n",
      "Epoch: 105, Train MSE: 0.1295, Test MSE: 0.1141\n",
      "Epoch: 106, Train MSE: 0.1295, Test MSE: 0.1136\n",
      "Epoch: 107, Train MSE: 0.1294, Test MSE: 0.1155\n",
      "Epoch: 108, Train MSE: 0.1293, Test MSE: 0.1154\n",
      "Epoch: 109, Train MSE: 0.1312, Test MSE: 0.1220\n",
      "Epoch: 110, Train MSE: 0.1295, Test MSE: 0.1131\n",
      "Epoch: 111, Train MSE: 0.1297, Test MSE: 0.1181\n",
      "Epoch: 112, Train MSE: 0.1295, Test MSE: 0.1171\n",
      "Epoch: 113, Train MSE: 0.1292, Test MSE: 0.1138\n",
      "Epoch: 114, Train MSE: 0.1292, Test MSE: 0.1136\n",
      "Epoch: 115, Train MSE: 0.1292, Test MSE: 0.1138\n",
      "Epoch: 116, Train MSE: 0.1296, Test MSE: 0.1183\n",
      "Epoch: 117, Train MSE: 0.1297, Test MSE: 0.1122\n",
      "Epoch: 118, Train MSE: 0.1293, Test MSE: 0.1130\n",
      "Epoch: 119, Train MSE: 0.1291, Test MSE: 0.1135\n",
      "Epoch: 120, Train MSE: 0.1306, Test MSE: 0.1214\n",
      "Epoch: 121, Train MSE: 0.1307, Test MSE: 0.1112\n",
      "Epoch: 122, Train MSE: 0.1292, Test MSE: 0.1173\n",
      "Epoch: 123, Train MSE: 0.1315, Test MSE: 0.1109\n",
      "Epoch: 124, Train MSE: 0.1294, Test MSE: 0.1184\n",
      "Epoch: 125, Train MSE: 0.1294, Test MSE: 0.1122\n",
      "Epoch: 126, Train MSE: 0.1296, Test MSE: 0.1192\n",
      "Epoch: 127, Train MSE: 0.1288, Test MSE: 0.1136\n",
      "Epoch: 128, Train MSE: 0.1287, Test MSE: 0.1147\n",
      "Epoch: 129, Train MSE: 0.1294, Test MSE: 0.1120\n",
      "Epoch: 130, Train MSE: 0.1291, Test MSE: 0.1181\n",
      "Epoch: 131, Train MSE: 0.1292, Test MSE: 0.1123\n",
      "Epoch: 132, Train MSE: 0.1294, Test MSE: 0.1193\n",
      "Epoch: 133, Train MSE: 0.1288, Test MSE: 0.1173\n",
      "Epoch: 134, Train MSE: 0.1285, Test MSE: 0.1150\n",
      "Epoch: 135, Train MSE: 0.1289, Test MSE: 0.1125\n",
      "Epoch: 136, Train MSE: 0.1285, Test MSE: 0.1138\n",
      "Epoch: 137, Train MSE: 0.1288, Test MSE: 0.1125\n",
      "Epoch: 138, Train MSE: 0.1309, Test MSE: 0.1235\n",
      "Epoch: 139, Train MSE: 0.1290, Test MSE: 0.1121\n",
      "Epoch: 140, Train MSE: 0.1302, Test MSE: 0.1220\n",
      "Epoch: 141, Train MSE: 0.1283, Test MSE: 0.1149\n",
      "Epoch: 142, Train MSE: 0.1289, Test MSE: 0.1186\n",
      "Epoch: 143, Train MSE: 0.1300, Test MSE: 0.1111\n",
      "Epoch: 144, Train MSE: 0.1286, Test MSE: 0.1177\n",
      "Epoch: 145, Train MSE: 0.1286, Test MSE: 0.1124\n",
      "Epoch: 146, Train MSE: 0.1281, Test MSE: 0.1144\n",
      "Epoch: 147, Train MSE: 0.1281, Test MSE: 0.1156\n",
      "Epoch: 148, Train MSE: 0.1282, Test MSE: 0.1165\n",
      "Epoch: 149, Train MSE: 0.1280, Test MSE: 0.1136\n",
      "Epoch: 150, Train MSE: 0.1285, Test MSE: 0.1123\n",
      "Epoch: 151, Train MSE: 0.1280, Test MSE: 0.1156\n",
      "Epoch: 152, Train MSE: 0.1279, Test MSE: 0.1141\n",
      "Epoch: 153, Train MSE: 0.1279, Test MSE: 0.1135\n",
      "Epoch: 154, Train MSE: 0.1282, Test MSE: 0.1172\n",
      "Epoch: 155, Train MSE: 0.1278, Test MSE: 0.1144\n",
      "Epoch: 156, Train MSE: 0.1296, Test MSE: 0.1110\n",
      "Epoch: 157, Train MSE: 0.1283, Test MSE: 0.1183\n",
      "Epoch: 158, Train MSE: 0.1287, Test MSE: 0.1116\n",
      "Epoch: 159, Train MSE: 0.1276, Test MSE: 0.1146\n",
      "Epoch: 160, Train MSE: 0.1278, Test MSE: 0.1129\n",
      "Epoch: 161, Train MSE: 0.1279, Test MSE: 0.1127\n",
      "Epoch: 162, Train MSE: 0.1283, Test MSE: 0.1188\n",
      "Epoch: 163, Train MSE: 0.1275, Test MSE: 0.1138\n",
      "Epoch: 164, Train MSE: 0.1276, Test MSE: 0.1133\n",
      "Epoch: 165, Train MSE: 0.1275, Test MSE: 0.1162\n",
      "Epoch: 166, Train MSE: 0.1275, Test MSE: 0.1136\n",
      "Epoch: 167, Train MSE: 0.1276, Test MSE: 0.1167\n",
      "Epoch: 168, Train MSE: 0.1278, Test MSE: 0.1124\n",
      "Epoch: 169, Train MSE: 0.1274, Test MSE: 0.1161\n",
      "Epoch: 170, Train MSE: 0.1276, Test MSE: 0.1169\n",
      "Epoch: 171, Train MSE: 0.1274, Test MSE: 0.1162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 172, Train MSE: 0.1296, Test MSE: 0.1108\n",
      "Epoch: 173, Train MSE: 0.1279, Test MSE: 0.1186\n",
      "Epoch: 174, Train MSE: 0.1273, Test MSE: 0.1137\n",
      "Epoch: 175, Train MSE: 0.1277, Test MSE: 0.1122\n",
      "Epoch: 176, Train MSE: 0.1272, Test MSE: 0.1156\n",
      "Epoch: 177, Train MSE: 0.1274, Test MSE: 0.1126\n",
      "Epoch: 178, Train MSE: 0.1279, Test MSE: 0.1192\n",
      "Epoch: 179, Train MSE: 0.1297, Test MSE: 0.1107\n",
      "Epoch: 180, Train MSE: 0.1270, Test MSE: 0.1150\n",
      "Epoch: 181, Train MSE: 0.1270, Test MSE: 0.1159\n",
      "Epoch: 182, Train MSE: 0.1282, Test MSE: 0.1113\n",
      "Epoch: 183, Train MSE: 0.1276, Test MSE: 0.1185\n",
      "Epoch: 184, Train MSE: 0.1269, Test MSE: 0.1134\n",
      "Epoch: 185, Train MSE: 0.1268, Test MSE: 0.1141\n",
      "Epoch: 186, Train MSE: 0.1270, Test MSE: 0.1131\n",
      "Epoch: 187, Train MSE: 0.1276, Test MSE: 0.1193\n",
      "Epoch: 188, Train MSE: 0.1269, Test MSE: 0.1129\n",
      "Epoch: 189, Train MSE: 0.1267, Test MSE: 0.1146\n",
      "Epoch: 190, Train MSE: 0.1281, Test MSE: 0.1208\n",
      "Epoch: 191, Train MSE: 0.1266, Test MSE: 0.1145\n",
      "Epoch: 192, Train MSE: 0.1270, Test MSE: 0.1125\n",
      "Epoch: 193, Train MSE: 0.1267, Test MSE: 0.1133\n",
      "Epoch: 194, Train MSE: 0.1266, Test MSE: 0.1142\n",
      "Epoch: 195, Train MSE: 0.1273, Test MSE: 0.1118\n",
      "Epoch: 196, Train MSE: 0.1266, Test MSE: 0.1163\n",
      "Epoch: 197, Train MSE: 0.1277, Test MSE: 0.1113\n",
      "Epoch: 198, Train MSE: 0.1266, Test MSE: 0.1164\n",
      "Epoch: 199, Train MSE: 0.1265, Test MSE: 0.1163\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import Javascript\n",
    "# display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()###torch.nn.CrossEntropyLoss()\n",
    "best_loss = float('inf')\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index, data.batch).float().squeeze()\n",
    "        #  print(type(out[0]))  # Perform a single forward pass.\n",
    "        #  print(data.y.shape)\n",
    "        loss = criterion(out.to(torch.float), data.y.float())  # Compute the loss.\n",
    "        #  print(loss)\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    # criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.batch).float().squeeze()\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            total_samples += data.num_graphs\n",
    "\n",
    "    mse = total_loss / total_samples\n",
    "    return mse\n",
    "  \n",
    "for epoch in range(1, 200):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train MSE: {train_acc:.4f}, Test MSE: {test_acc:.4f}')\n",
    "    if test_acc < best_loss:\n",
    "        best_loss = test_acc\n",
    "        torch.save(model.state_dict(), 'best_model_1week_Nhops.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bab5ce7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7264"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
