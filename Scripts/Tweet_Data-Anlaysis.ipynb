{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb90acc",
   "metadata": {},
   "source": [
    "## Helper_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f65fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324dd1ca",
   "metadata": {},
   "source": [
    "## Library Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a5e89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447a2e1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5757f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pickle_path = '../Pickle_Files/'\n",
    "csv_path = '../Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e0d63c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pickle_path+\"master_df_noNa.pickle\",'rb') as fin:\n",
    "    master_df = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "593240a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['hashtag_list'] = master_df['tw_text'].apply(extract_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "477a0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['tweet_dt'] = master_df['tw_time'].apply(get_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e27da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(master_df)\n",
    "# 860485\n",
    "\n",
    "master_df_nodup = master_df.drop_duplicates(['tw_text'])\n",
    "\n",
    "# len(master_df_nodup)\n",
    "# 333116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15311d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup_usa = master_df_nodup[master_df_nodup['usa_flg']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356d2518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tw_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_dt</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-05-27</th>\n",
       "      <td>3033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-28</th>\n",
       "      <td>3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-29</th>\n",
       "      <td>2730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-30</th>\n",
       "      <td>1820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-31</th>\n",
       "      <td>1211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tw_id\n",
       "tweet_dt         \n",
       "2020-05-27   3033\n",
       "2020-05-28   3052\n",
       "2020-05-29   2730\n",
       "2020-05-30   1820\n",
       "2020-05-31   1211"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_nodup_usa.groupby(['tweet_dt']).agg({'tw_id':'count'}).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239804eb",
   "metadata": {},
   "source": [
    "### Lets clean the tweet text for running \n",
    "1. Sentiment Analysis\n",
    "2. Stance Detection wrt Covid -19 \n",
    "3. EMotion extraction for 11 dimensions\n",
    "4. COnvert text into high dim vector represenation using BERT sentence encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36928662",
   "metadata": {},
   "source": [
    "#### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710fd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes special characters (except commas and spaces), URLs, mentions, and hashtags from a tweet text\n",
    "\n",
    "df_nodup_usa['twt_clean'] = df_nodup_usa['tw_text'].apply(clean_tweet_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ade320",
   "metadata": {},
   "source": [
    "### Adding Binary Sentiment Score to tweet using \n",
    "- distilbert-base-uncased-finetuned-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "187f7ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e088f2d9",
   "metadata": {},
   "source": [
    "### Sentiment Extraction using BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78a0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### we need tweet list for running tasks like emotion, stance, topic modeling. so lets just make a list\n",
    "tweets_list = df_nodup_usa['twt_clean'].to_list()\n",
    "\n",
    "\n",
    "with open(Pickle_path+\"tweets_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tweets_list, f)\n",
    "\n",
    "#Example\n",
    "sentiment_tokenizer_model = 'distilbert-base-uncased'\n",
    "sentiment_model = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "result=add_sentiment(tweets_list,sentiment_tokenizer_model,sentiment_model)\n",
    "\n",
    "\n",
    "with open(Pickle_path+\"sentiment_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d9aa7",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis done\n",
    "    - output stored in Pickle_path+\"sentiment_results.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ce6d3",
   "metadata": {},
   "source": [
    "### Sentence Encoder to encode sentences into Vectors\n",
    "    -SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    -spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5f3bff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 101659/101659 [13:22<00:00, 126.64it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "data_embeddings = []\n",
    "for des in tqdm(tweets_list):\n",
    "    embeddings = model.encode(des,)\n",
    "    data_embeddings.append(embeddings)\n",
    "    \n",
    "embeddings_output = data_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "718d1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(Pickle_path+\"embeddings_output_tweets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_output, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9f2a13",
   "metadata": {},
   "source": [
    "##### Sentence encoding to 364 dim vectors done\n",
    "    - Output saved in Pickle_path+\"embeddings_output_tweets.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344ec51",
   "metadata": {},
   "source": [
    "### Stance Detection Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this, we can load Python file \n",
    "# !python Stance_Detection_Module.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445bbdf3",
   "metadata": {},
   "source": [
    "##### Stance Detection Module finished. \n",
    "    - Output avaialble in Pickle_path+\"Stance_predictions_list.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909a7cc3",
   "metadata": {},
   "source": [
    "### Emotion Extraction Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3029990",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_input_df = pd.DataFrame(tweets_list,columns=['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c06e1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cols = [\"id\",\"anger\",\"anticipation\",\"disgust\",\"fear\",\"joy\",\"love\",\"optimism\",\"pessimism\",\"sadness\",\"surprise\",\"trust\"]\n",
    "emotion_input_df[dummy_cols] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ae2d8534",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_input_df.to_csv(\"modify_data.csv\",index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26ecb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(Pickle_path+\"emotion_input_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(emotion_input_df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80dfcb",
   "metadata": {},
   "source": [
    "##### After this, Run the code on serve given my Menguxn on the file generated emotion_input_df.pkl\n",
    "\n",
    "#### It generates a csv file with emotion scores. \n",
    "\n",
    "    -Output available as \n",
    "    -emotion_scores_output = pd.read_csv(csv_path+\"out_put.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33784f",
   "metadata": {},
   "source": [
    "### Hashtag Vector Generation Module\n",
    "    - We only focus on top 1000 hashtags to ensure vector size in under managabele size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e8a5cff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt_url</th>\n",
       "      <th>tw_id</th>\n",
       "      <th>hashtag_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101649</th>\n",
       "      <td>https://conta.cc/2YGSqk0</td>\n",
       "      <td>1.258358e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101650</th>\n",
       "      <td>https://twitter.com/nytimes/status/12595110326...</td>\n",
       "      <td>1.259571e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101651</th>\n",
       "      <td>https://trib.al/BPUuFW4</td>\n",
       "      <td>1.259567e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101652</th>\n",
       "      <td>https://www.news5cleveland.com/news/local-news...</td>\n",
       "      <td>1.259573e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101653</th>\n",
       "      <td>https://lizditz.typepad.com/i_speak_of_dreams/...</td>\n",
       "      <td>1.259564e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101654</th>\n",
       "      <td>http://ow.ly/Jkpn50zAi9c</td>\n",
       "      <td>1.259507e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101655</th>\n",
       "      <td>https://twitter.com/egheitasean/status/1259488...</td>\n",
       "      <td>1.259503e+18</td>\n",
       "      <td>(#maca,)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101656</th>\n",
       "      <td>https://www.wsj.com/articles/the-economic-lock...</td>\n",
       "      <td>1.259135e+18</td>\n",
       "      <td>(#lalege, #lagov)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>http://f-st.co/kLiCICJ</td>\n",
       "      <td>1.259573e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101658</th>\n",
       "      <td>https://www.vox.com/2020/5/10/21252583/coronav...</td>\n",
       "      <td>1.259557e+18</td>\n",
       "      <td>()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   rt_url         tw_id  \\\n",
       "101649                           https://conta.cc/2YGSqk0  1.258358e+18   \n",
       "101650  https://twitter.com/nytimes/status/12595110326...  1.259571e+18   \n",
       "101651                            https://trib.al/BPUuFW4  1.259567e+18   \n",
       "101652  https://www.news5cleveland.com/news/local-news...  1.259573e+18   \n",
       "101653  https://lizditz.typepad.com/i_speak_of_dreams/...  1.259564e+18   \n",
       "101654                           http://ow.ly/Jkpn50zAi9c  1.259507e+18   \n",
       "101655  https://twitter.com/egheitasean/status/1259488...  1.259503e+18   \n",
       "101656  https://www.wsj.com/articles/the-economic-lock...  1.259135e+18   \n",
       "101657                             http://f-st.co/kLiCICJ  1.259573e+18   \n",
       "101658  https://www.vox.com/2020/5/10/21252583/coronav...  1.259557e+18   \n",
       "\n",
       "             hashtag_list  \n",
       "101649                 ()  \n",
       "101650                 ()  \n",
       "101651                 ()  \n",
       "101652                 ()  \n",
       "101653                 ()  \n",
       "101654                 ()  \n",
       "101655           (#maca,)  \n",
       "101656  (#lalege, #lagov)  \n",
       "101657                 ()  \n",
       "101658                 ()  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MasterDf[['rt_url','tw_id','hashtag_list']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "06d274f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in your master data DataFrame\n",
    "master_data_df = MasterDf[:]\n",
    "\n",
    "\n",
    "\n",
    "# Get the top 10,000 hashtags by frequency\n",
    "all_hashtags = [hashtag for hashtags in master_data_df['hashtag_list'] for hashtag in hashtags]\n",
    "top_hashtags = pd.Series(all_hashtags).value_counts()[:1000].index.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Filter the hashtag lists in your DataFrame to only include the top 10,000 hashtags\n",
    "filtered_hashtag_lists = [[hashtag for hashtag in hashtags if hashtag in top_hashtags] for hashtags in master_data_df['hashtag_list']]\n",
    "\n",
    "# Convert the filtered hashtag lists to a one-hot encoded matrix\n",
    "mlb = MultiLabelBinarizer()\n",
    "one_hot_matrix = mlb.fit_transform(filtered_hashtag_lists)\n",
    "\n",
    "# Create a new DataFrame with the one-hot encoded hashtags\n",
    "# Create a new DataFrame with a single column of 10,000-element lists of encoded values\n",
    "encoded_hashtags = []\n",
    "for row in one_hot_matrix:\n",
    "    encoded_hashtags.append(row.tolist())\n",
    "\n",
    "encoded_hashtags_df = pd.DataFrame({'encoded_hashtags': encoded_hashtags})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_nodup_usa_hashtgs = pd.concat([master_data_df.reset_index(),encoded_hashtags_df],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "fcfa2881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101654</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101655</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101656</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101657</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101658</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101659 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         encoded_hashtags\n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "3       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "4       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "...                                                   ...\n",
       "101654  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "101655  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "101656  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "101657  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "101658  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[101659 rows x 1 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_hashtags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pickle_path+\"encoded_hashtags_df.pkl\", \"wb\") as f:\n",
    "    pickle.dump(encoded_hashtags_df, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd92141",
   "metadata": {},
   "source": [
    "#### Hashtag Output saved in \n",
    "    - Pickle_path+\"encoded_hashtags_df.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c16499",
   "metadata": {},
   "source": [
    "### Topic Modeling Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1a61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the tweets by removing stop words and tokenizing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tweets_df = pd.DataFrame(tweets_list,columns = ['tw_text'])\n",
    "tweets_df['tokens'] = tweets_df['tw_text'].apply(lambda x: [w.lower() for w in word_tokenize(x) if w.lower() not in stop_words])\n",
    "\n",
    "# Convert the tweets into a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tweets_df['tokens'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train an LDA model on the bag-of-words matrix\n",
    "lda_model = LatentDirichletAllocation(n_components=10, max_iter=100, random_state=42)\n",
    "lda_model.fit(X)\n",
    "\n",
    "# Print the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "\n",
    "# Assign each tweet a distribution over the topics\n",
    "topic_distributions = lda_model.transform(X)\n",
    "tweets_df['topic_distribution'] = list(topic_distributions)\n",
    "\n",
    "# Map the top topic for each tweet to a semantic category\n",
    "topic_labels = {\n",
    "    0: 'politics',\n",
    "    1: 'public health',\n",
    "    2: 'vaccination',\n",
    "    3: 'epidemiology',\n",
    "    4: 'social distancing',\n",
    "    5: 'mental health',\n",
    "    6: 'pandemic response',\n",
    "    7: 'testing',\n",
    "    8: 'treatment',\n",
    "    9: 'communication'\n",
    "}\n",
    "tweets_df['top_topic'] = tweets_df['topic_distribution'].apply(lambda x: np.argmax(x))\n",
    "tweets_df['semantic_category'] = tweets_df['top_topic'].map(topic_labels)\n",
    "\n",
    "\n",
    "\n",
    "with open(Pickle_path+\"Topic_modelling_results.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(tweets_df,topic_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f34d38",
   "metadata": {},
   "source": [
    "\n",
    "#### Topic Modeling output available in \n",
    "    - Pickle_path+\"Topic_modelling_results.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "479c3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Also saving master dataframe after reindexing \n",
    "\n",
    "with open(Pickle_path+\"df_nodup_usa.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(df_nodup_usa.reset_index(),topic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "adf82afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sentiment_score_results.csv out_put.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f1602b",
   "metadata": {},
   "source": [
    "#### Now we have outputs from \n",
    "    - Sentiment Analysis at sentiment_results.pkl\n",
    "    - Emotion Analysis at ../Data/out_put.csv\n",
    "    - Stance wrt Covid-19 at Stance_predictions_list.pkl\n",
    "    - Hashtag vector at encoded_hashtags_df.pkl\n",
    "    - Topic Theme Vectors at Topic_modelling_results.pkl\n",
    "    - Vector Encodings at embeddings_output_tweets.pkl\n",
    "\n",
    "#### Master DataFrame is available at \n",
    "    - Pickle_path+\"df_nodup_usa.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abbed23",
   "metadata": {},
   "source": [
    "### Loading outputs from all modules into current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c8f5a",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1ee98ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-99.427658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-99.571890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment\n",
       "0 -99.427658\n",
       "1 -99.571890"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_results = pickle_loader(Pickle_path+\"sentiment_results.pkl\")\n",
    "\n",
    "sentiment_results_subset = sentiment_results[['sentiment']]\n",
    "\n",
    "sentiment_results_subset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1ac4b",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "01646329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contradiction</th>\n",
       "      <th>entailment</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999482</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.994533</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contradiction  entailment   neutral\n",
       "0       0.999482    0.000022  0.000496\n",
       "1       0.994533    0.000001  0.005466"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stance_results_tmp = pickle_loader(Pickle_path+\"Stance_predictions_list.pkl\")\n",
    "\n",
    "stance_results = []\n",
    "\n",
    "for block in stance_results_tmp:\n",
    "    for row in block:\n",
    "        stance_results.append(row)\n",
    "        \n",
    "\n",
    "\n",
    "stance_results_df = pd.DataFrame(stance_results,columns = [\"contradiction\", \"entailment\", \"neutral\"])\n",
    "\n",
    "stance_output_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bddb3",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "196fed1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_results = pd.read_csv(\"../Data/out_put.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f7d7f3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641829</td>\n",
       "      <td>0.149830</td>\n",
       "      <td>0.701071</td>\n",
       "      <td>0.024524</td>\n",
       "      <td>0.029221</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.025946</td>\n",
       "      <td>0.069477</td>\n",
       "      <td>0.251814</td>\n",
       "      <td>0.025928</td>\n",
       "      <td>0.005484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236262</td>\n",
       "      <td>0.244001</td>\n",
       "      <td>0.745701</td>\n",
       "      <td>0.350815</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.006780</td>\n",
       "      <td>0.094898</td>\n",
       "      <td>0.291147</td>\n",
       "      <td>0.142205</td>\n",
       "      <td>0.005690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018904</td>\n",
       "      <td>0.533875</td>\n",
       "      <td>0.048713</td>\n",
       "      <td>0.003294</td>\n",
       "      <td>0.832357</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.788394</td>\n",
       "      <td>0.007254</td>\n",
       "      <td>0.016247</td>\n",
       "      <td>0.131739</td>\n",
       "      <td>0.121118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.206196</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>0.287812</td>\n",
       "      <td>0.017970</td>\n",
       "      <td>0.631388</td>\n",
       "      <td>0.014190</td>\n",
       "      <td>0.049942</td>\n",
       "      <td>0.024464</td>\n",
       "      <td>0.234475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035117</td>\n",
       "      <td>0.149447</td>\n",
       "      <td>0.260748</td>\n",
       "      <td>0.179213</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>0.173826</td>\n",
       "      <td>0.478952</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>0.004780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.027669</td>\n",
       "      <td>0.474313</td>\n",
       "      <td>0.100259</td>\n",
       "      <td>0.084536</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.049969</td>\n",
       "      <td>0.026967</td>\n",
       "      <td>0.060703</td>\n",
       "      <td>0.212374</td>\n",
       "      <td>0.014591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.006425</td>\n",
       "      <td>0.757777</td>\n",
       "      <td>0.020197</td>\n",
       "      <td>0.068641</td>\n",
       "      <td>0.148025</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.766238</td>\n",
       "      <td>0.015592</td>\n",
       "      <td>0.027163</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.081728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.339228</td>\n",
       "      <td>0.438487</td>\n",
       "      <td>0.651328</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.021179</td>\n",
       "      <td>0.003116</td>\n",
       "      <td>0.015219</td>\n",
       "      <td>0.027993</td>\n",
       "      <td>0.081596</td>\n",
       "      <td>0.240584</td>\n",
       "      <td>0.010672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.134228</td>\n",
       "      <td>0.190615</td>\n",
       "      <td>0.405692</td>\n",
       "      <td>0.067951</td>\n",
       "      <td>0.019809</td>\n",
       "      <td>0.002650</td>\n",
       "      <td>0.032220</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>0.838388</td>\n",
       "      <td>0.019853</td>\n",
       "      <td>0.003878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.753092</td>\n",
       "      <td>0.044671</td>\n",
       "      <td>0.914730</td>\n",
       "      <td>0.941997</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>0.091530</td>\n",
       "      <td>0.285891</td>\n",
       "      <td>0.016644</td>\n",
       "      <td>0.004070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  anticipation   disgust      fear       joy      love  optimism  \\\n",
       "0  0.641829      0.149830  0.701071  0.024524  0.029221  0.002634  0.025946   \n",
       "1  0.236262      0.244001  0.745701  0.350815  0.003112  0.001413  0.006780   \n",
       "2  0.018904      0.533875  0.048713  0.003294  0.832357  0.032280  0.788394   \n",
       "3  0.006553      0.206196  0.016599  0.016862  0.287812  0.017970  0.631388   \n",
       "4  0.035117      0.149447  0.260748  0.179213  0.005718  0.001923  0.017826   \n",
       "5  0.027669      0.474313  0.100259  0.084536  0.020202  0.002475  0.049969   \n",
       "6  0.006425      0.757777  0.020197  0.068641  0.148025  0.002728  0.766238   \n",
       "7  0.339228      0.438487  0.651328  0.056200  0.021179  0.003116  0.015219   \n",
       "8  0.134228      0.190615  0.405692  0.067951  0.019809  0.002650  0.032220   \n",
       "9  0.753092      0.044671  0.914730  0.941997  0.004812  0.001419  0.008310   \n",
       "\n",
       "   pessimism   sadness  surprise     trust  \n",
       "0   0.069477  0.251814  0.025928  0.005484  \n",
       "1   0.094898  0.291147  0.142205  0.005690  \n",
       "2   0.007254  0.016247  0.131739  0.121118  \n",
       "3   0.014190  0.049942  0.024464  0.234475  \n",
       "4   0.173826  0.478952  0.028044  0.004780  \n",
       "5   0.026967  0.060703  0.212374  0.014591  \n",
       "6   0.015592  0.027163  0.034309  0.081728  \n",
       "7   0.027993  0.081596  0.240584  0.010672  \n",
       "8   0.207090  0.838388  0.019853  0.003878  \n",
       "9   0.091530  0.285891  0.016644  0.004070  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc36a9f5",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7f8ef86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    encoded_hashtags\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ..."
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_results = pickle_loader(Pickle_path+\"encoded_hashtags_df.pkl\")\n",
    "\n",
    "hashtag_results.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ececcf21",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b2100a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_themes_df = pickle_loader(Pickle_path+\"Topic_modelling_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e4dab4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_distribution</th>\n",
       "      <th>top_topic</th>\n",
       "      <th>semantic_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.025002401429176736, 0.02500000005850058, 0....</td>\n",
       "      <td>4</td>\n",
       "      <td>social distancing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.011111892748436235, 0.011112492275769222, 0...</td>\n",
       "      <td>8</td>\n",
       "      <td>treatment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  topic_distribution  top_topic  \\\n",
       "0  [0.025002401429176736, 0.02500000005850058, 0....          4   \n",
       "1  [0.011111892748436235, 0.011112492275769222, 0...          8   \n",
       "\n",
       "   semantic_category  \n",
       "0  social distancing  \n",
       "1          treatment  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Topic_themes_df_sbset = Topic_themes_df[['topic_distribution','top_topic','semantic_category']]\n",
    "Topic_themes_df_sbset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8126b",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "7e6f7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_embedding_output = pickle_loader(Pickle_path+\"embeddings_output_tweets.pkl\")\n",
    "\n",
    "\n",
    "Sentence_embedding_output_df = pd.DataFrame([Sentence_embedding_output]).T\n",
    "\n",
    "Sentence_embedding_output_df.columns = ['Sentence_Embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "31879590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.027914412, 0.015142873, 0.015514709, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.004281366, 0.043223437, 0.037251867, 0.038...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Sentence_Embedding\n",
       "0  [-0.027914412, 0.015142873, 0.015514709, -0.05...\n",
       "1  [-0.004281366, 0.043223437, 0.037251867, 0.038..."
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentence_embedding_output_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "721fe5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MasterDf = pickle_loader(Pickle_path+\"df_nodup_usa.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb898ae5",
   "metadata": {},
   "source": [
    "### Merge All above dataframe to master df to obtain tweet level Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "bdd6a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_lvl_master_df = pd.concat([MasterDf,sentiment_results_subset,stance_output_df,emotion_results,\\\n",
    "          hashtag_results,Topic_themes_df_sbset,Sentence_embedding_output_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "5377322c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rtw_time</th>\n",
       "      <th>tw_time</th>\n",
       "      <th>retweet_cnt</th>\n",
       "      <th>rtw_usr_screen</th>\n",
       "      <th>tw_usr_screen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sun May 24 02:09:50 +0000 2020</td>\n",
       "      <td>Sat May 23 23:49:25 +0000 2020</td>\n",
       "      <td>49</td>\n",
       "      <td>s_derrickson</td>\n",
       "      <td>Rschooley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32198</th>\n",
       "      <td>Sat May 23 02:14:50 +0000 2020</td>\n",
       "      <td>Fri May 22 21:56:55 +0000 2020</td>\n",
       "      <td>58</td>\n",
       "      <td>stmichaelseesme</td>\n",
       "      <td>DeanBaker13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37340</th>\n",
       "      <td>Fri May 22 21:54:54 +0000 2020</td>\n",
       "      <td>Fri May 22 20:50:10 +0000 2020</td>\n",
       "      <td>14</td>\n",
       "      <td>MOEInvestigator</td>\n",
       "      <td>ArthurCaplan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37341</th>\n",
       "      <td>Fri May 22 21:54:58 +0000 2020</td>\n",
       "      <td>Fri May 22 21:42:29 +0000 2020</td>\n",
       "      <td>10</td>\n",
       "      <td>nanmac321</td>\n",
       "      <td>BlueSt0rmRising</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89366</th>\n",
       "      <td>Sat May 23 00:42:01 +0000 2020</td>\n",
       "      <td>Sat May 23 00:31:56 +0000 2020</td>\n",
       "      <td>8</td>\n",
       "      <td>Avonan</td>\n",
       "      <td>juliabhaber</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             rtw_time                         tw_time  \\\n",
       "0      Sun May 24 02:09:50 +0000 2020  Sat May 23 23:49:25 +0000 2020   \n",
       "32198  Sat May 23 02:14:50 +0000 2020  Fri May 22 21:56:55 +0000 2020   \n",
       "37340  Fri May 22 21:54:54 +0000 2020  Fri May 22 20:50:10 +0000 2020   \n",
       "37341  Fri May 22 21:54:58 +0000 2020  Fri May 22 21:42:29 +0000 2020   \n",
       "89366  Sat May 23 00:42:01 +0000 2020  Sat May 23 00:31:56 +0000 2020   \n",
       "\n",
       "       retweet_cnt   rtw_usr_screen    tw_usr_screen  \n",
       "0               49     s_derrickson        Rschooley  \n",
       "32198           58  stmichaelseesme      DeanBaker13  \n",
       "37340           14  MOEInvestigator     ArthurCaplan  \n",
       "37341           10        nanmac321  BlueSt0rmRising  \n",
       "89366            8           Avonan      juliabhaber  "
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_url = 'https://www.cnn.com/2020/05/22/investing/moderna-coronavirus-vaccine-stock-sales/index.html'\n",
    "\n",
    "Tweet_lvl_master_df[Tweet_lvl_master_df['rt_url']==tmp_url][['rtw_time','tw_time','retweet_cnt','rtw_usr_screen','tw_usr_screen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "a98d127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(Pickle_path+\"Tweet_lvl_master_df.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(Tweet_lvl_master_df,topic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ec5d5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Org_tweets_df = Tweet_lvl_master_df[['rt_url','tw_id', 'tw_time', 'tw_text', 'tw_usr_screen', 'usr_follow_cnt','tweet_dt', 'twt_clean', 'sentiment', 'contradiction',\n",
    "       'entailment', 'neutral', 'anger', 'anticipation', 'disgust', 'fear',\n",
    "       'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust',\n",
    "       'encoded_hashtags', 'topic_distribution', 'top_topic',\n",
    "       'semantic_category', 'Sentence_Embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b1647740",
   "metadata": {},
   "outputs": [],
   "source": [
    "Org_tweets_df = Org_tweets_df.drop_duplicates(['tw_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "3b913a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Org_tweets_df['Tweet_type'] = 'Org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "003a5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(Pickle_path+\"Org_tweets_df.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(Org_tweets_df,topic_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "7087c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df = Tweet_lvl_master_df[['rt_url','rtw_id', 'rtw_time', 'rtw_text', 'rtw_usr_screen', 'rusr_follow_cnt', 'twt_clean', 'sentiment', 'contradiction',\n",
    "       'entailment', 'neutral', 'anger', 'anticipation', 'disgust', 'fear',\n",
    "       'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust',\n",
    "       'encoded_hashtags', 'topic_distribution', 'top_topic',\n",
    "       'semantic_category', 'Sentence_Embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "d8cd61f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df['rtweet_dt'] = retwt_df['rtw_time'].apply(get_date)\n",
    "\n",
    "retwt_df = retwt_df.drop_duplicates(['rtw_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "0a963eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df['Tweet_type'] = 'Retweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "9f41e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(Pickle_path+\"retwt_df.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(retwt_df,topic_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2601fe9",
   "metadata": {},
   "source": [
    "\n",
    "#### Merge tweet, retweet dataframes to create master df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "94b8b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df_1 = retwt_df[['rt_url','rtw_id', 'rtw_time', 'rtw_text', 'rtw_usr_screen', 'rusr_follow_cnt',\\\n",
    "        'rtweet_dt', 'twt_clean','sentiment', 'contradiction', 'entailment', 'neutral',\n",
    "       'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism',\n",
    "       'pessimism', 'sadness', 'surprise', 'trust', 'encoded_hashtags',\n",
    "       'topic_distribution', 'top_topic', 'semantic_category',\n",
    "       'Sentence_Embedding','Tweet_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "716b6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df_1.columns = list(Org_tweets_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26c9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "retwt_df_1['tw_usr_screen'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "5a622b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### below dataframe is at tweet_id Level\n",
    "\n",
    "twt_tetwt_master_df = pd.concat([Org_tweets_df,retwt_df_1],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d939fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(Pickle_path+\"twt_tetwt_master_df.pkl\",'wb') as topic_out:\n",
    "    pickle.dump(twt_tetwt_master_df,topic_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96d46e",
   "metadata": {},
   "source": [
    "### Roll Up Data at User Level to create individual user Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "f78a4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodup_usa_hashtgs['usr_follow_cnt'] = df_nodup_usa_hashtgs['usr_follow_cnt'].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "097939ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in your df_nodup_usa_hashtgs DataFrame\n",
    "df_nodup_usa_hashtgs = twt_tetwt_master_df[:]\n",
    "\n",
    "# Group by tw_usr_screen and aggregate the columns as specified\n",
    "grouped_df = df_nodup_usa_hashtgs.groupby('tw_usr_screen').agg({\n",
    "    'encoded_hashtags': lambda x: [sum(val) for val in zip(*x)],   #### taking sum for hashtags\n",
    "    'topic_distribution': lambda x: [sum(val)/len(val) for val in zip(*x)],#, ## for rest, taking mean\n",
    "    'Sentence_Embedding': lambda x: [sum(val)/len(val) for val in zip(*x)],#,\n",
    "    'sentiment': 'mean',\n",
    "    'contradiction': 'mean',\n",
    "    'entailment': 'mean',\n",
    "    'neutral': 'mean',\n",
    "    'anger': 'mean',\n",
    "    'anticipation': 'mean',\n",
    "    'disgust': 'mean',\n",
    "    'fear': 'mean',\n",
    "    'joy': 'mean',\n",
    "    'love': 'mean',\n",
    "    'optimism': 'mean',\n",
    "    'pessimism': 'mean',\n",
    "    'sadness': 'mean',\n",
    "    'surprise': 'mean',\n",
    "    'trust': 'mean',\n",
    "    'rt_url':\"count\"\n",
    "#     ,\n",
    "#     'usr_follow_cnt':'max'\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8214035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'optimism',\n",
       " 'pessimism',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'trust']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list({    'anger': 'mean',\n",
    "    'anticipation': 'mean',\n",
    "    'disgust': 'mean',\n",
    "    'fear': 'mean',\n",
    "    'joy': 'mean',\n",
    "    'love': 'mean',\n",
    "    'optimism': 'mean',\n",
    "    'pessimism': 'mean',\n",
    "    'sadness': 'mean',\n",
    "    'surprise': 'mean',\n",
    "    'trust': 'mean'}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "4cac2aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df = grouped_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594972e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df.to_csv(csv_path+\"User_profile.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "55cba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df = pd.read_csv(csv_path+\"User_profile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f52c2199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5867"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "31e1648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df.sort_values(['rt_url'],ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "52aa60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "User_level_feat_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fdf9f28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [303], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [row[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m list_cols] \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_col\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/frame.py:9558\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9547\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m   9549\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m   9550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9551\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9556\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m   9557\u001b[0m )\n\u001b[0;32m-> 9558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/apply.py:741\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/apply.py:868\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 868\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/apply.py:884\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 884\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    886\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    887\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    888\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [303], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: [row[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_cols\u001b[49m] \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist_col\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_cols' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df['all_features'] = df.apply(lambda row: [row[col] for col in list_cols] + row['list_col'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "c52205b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols = ['contradiction', 'entailment', 'neutral', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "def concat_lists(row):\n",
    "    return row['encoded_hashtags'] + \\\n",
    "            row['topic_distribution']+[row[col] for col in list_cols] \n",
    "\n",
    "# apply the function to each row\n",
    "User_level_feat_df['Feature_set'] = User_level_feat_df.apply(concat_lists, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "09468ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df['hastag_sum'] = User_level_feat_df['encoded_hashtags'].apply(sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56251a4e",
   "metadata": {},
   "source": [
    "### Lets run KNN to identify 10 neighbours of each tweet based on semantic similarity \n",
    "    - Run KNN on sentence embeddings\n",
    "    - create a adjacency matrix where we get list of all neighbours which we will use to create our graph edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d128182b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Feature_set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Feature_set'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [304], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m User_feature_vector \u001b[38;5;241m=\u001b[39m User_level_feat_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_set\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/pyenv/env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Feature_set'"
     ]
    }
   ],
   "source": [
    "User_feature_vector = User_level_feat_df['Feature_set'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "daab9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_feature_vector = [np.array(w) for w in User_feature_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1bde1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assume your DataFrame is named df\n",
    "tw_usr_screen_dict = User_level_feat_df.reset_index().set_index('tw_usr_screen')['index'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "8677194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2_idx_hashmap = {}\n",
    "for key,val in tw_usr_screen_dict.items():\n",
    "    key_2_idx_hashmap[val] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "29cdf9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# assuming your array is named 'embedding_output'\n",
    "n_neighbors = 10\n",
    "knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "knn.fit(User_feature_vector)\n",
    "\n",
    "# get the indices of the 10 nearest neighbors for each row\n",
    "distances, indices = knn.kneighbors(User_feature_vector)\n",
    "\n",
    "# create an adjacency dictionary where the key is the row index and the value is a list of its 10 nearest neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "1599ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = {}\n",
    "for i in range(len(User_feature_vector)):\n",
    "    adj_dict[key_2_idx_hashmap[i]] = [key_2_idx_hashmap[w] for w in list(indices[i][1:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "1e1c911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# compute the cosine similarity between each row and its 10 nearest neighbors\n",
    "similarity_dict = {}\n",
    "for i in range(len(User_feature_vector)):\n",
    "    neighbor_indices = indices[i].flatten()\n",
    "    row_similarities = []\n",
    "    for ind in neighbor_indices[1:]:\n",
    "        row_similarities.append(cosine_similarity([User_feature_vector[i]], [User_feature_vector[ind]])[0][0])\n",
    "    similarity_dict[key_2_idx_hashmap[i]] = dict(zip([key_2_idx_hashmap[w] for w in list(indices[i][1:])], row_similarities))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "cc34a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Pickle_path+\"node_similarity_dict.pkl\",'wb') as node_similarity_file:\n",
    "    pickle.dump(similarity_dict,node_similarity_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "5cfcfc5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fox5dc': 0.9946963651017979,\n",
       " 'WGNNews': 0.9970530660452035,\n",
       " 'KDKA': 0.9856506816647358,\n",
       " 'AuthorJLLopez': 0.6319364127360368,\n",
       " 'ABC7NY': 0.9764324289051096,\n",
       " 'WMCActionNews5': 0.9786821664704085,\n",
       " 'ajc': 0.9481427513237088,\n",
       " 'wxyzdetroit': 0.9504871245739013,\n",
       " 'fox32news': 0.9551514801111138}"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_dict['thehill']\n",
    "# {'fox5dc': 0.9946963651017979,\n",
    "#  'WGNNews': 0.9970530660452035,\n",
    "#  'KDKA': 0.9856506816647358,\n",
    "#  'AuthorJLLopez': 0.6319364127360368,\n",
    "#  'ABC7NY': 0.9764324289051096,\n",
    "#  'WMCActionNews5': 0.9786821664704085,\n",
    "#  'ajc': 0.9481427513237088,\n",
    "#  'wxyzdetroit': 0.9504871245739013,\n",
    "#  'fox32news': 0.9551514801111138}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f42a8f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Follower_cnt_dict = df_nodup_usa_hashtgs[['tw_usr_screen','usr_follow_cnt']].set_index('tw_usr_screen').to_dict()['usr_follow_cnt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444d01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_download(Pickle_path+\"Follower_cnt_dict.pkl\",Follower_cnt_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628a1d9",
   "metadata": {},
   "source": [
    "### GNN + LSTM Module\n",
    "    -Inputs:\n",
    "        Adjacency Matrix\n",
    "        Node Feature Dictionary\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c9ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pickle_loader(Pickle_path+\"Tweet_lvl_master_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1979f8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'rtw_time', 'rtw_id', 'rtw_text', 'rtw_usr_screen',\n",
       "       'rusr_follow_cnt', 'retweet_cnt', 'rusa_flg', 'rt_url', 'tw_time',\n",
       "       'tw_id', 'tw_text', 'tw_usr_screen', 'usr_follow_cnt', 'usa_flg',\n",
       "       'hashtag_list', 'tweet_dt', 'twt_clean', 'sentiment', 'contradiction',\n",
       "       'entailment', 'neutral', 'anger', 'anticipation', 'disgust', 'fear',\n",
       "       'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust',\n",
       "       'encoded_hashtags', 'topic_distribution', 'top_topic',\n",
       "       'semantic_category', 'Sentence_Embedding'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e2d88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8effaa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "master_df['tw_time'] = pd.to_datetime(master_df['tw_time'], format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "808b2f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `master_df` is a pandas DataFrame with the columns you described, as well as a `rt_url` column\n",
    "\n",
    "# Group the DataFrame by `rt_url`\n",
    "groups = master_df.groupby('rt_url')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "29af990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, group in groups:\n",
    "#     # Sort the group by `tw_time`\n",
    "#     sorted_group = group.sort_values('tw_time')\n",
    "#     dd = sorted_group.head(100)\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f851d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for name, group in groups:\n",
    "    # Sort the group by `tw_time`\n",
    "    sorted_group = group.sort_values('tw_time')\n",
    "    \n",
    "    # Compute the cutoff time for the first 60% of the tweets\n",
    "    cutoff_time = sorted_group.iloc[int(0.6 * len(sorted_group))]['tw_time']\n",
    "    \n",
    "    # Select the rows that fall within the first 60% of the tweets\n",
    "    first_60_percent = sorted_group[sorted_group['tw_time'] <= cutoff_time]\n",
    "    first_60_percent_unq = first_60_percent.drop_duplicates(['tw_usr_screen'])\n",
    "    \n",
    "    # Aggregate the `retweet_cnt`, `rtw_usr_screen`, and `tw_usr_screen` columns of the selected rows\n",
    "#     first_60_percent_agg = {\n",
    "#         'rt_url': name,\n",
    "#         'retweet_cnt_sum_60': first_60_percent_unq['retweet_cnt'].sum(),\n",
    "#         'rtw_usr_screen_list_60': tuple(set(first_60_percent['rtw_usr_screen'].tolist())),\n",
    "#         'tw_usr_screen_list_60': tuple(set(first_60_percent['tw_usr_screen'].tolist()))\n",
    "#     }\n",
    "    \n",
    "    # Select all the rows\n",
    "    all_rows = sorted_group\n",
    "    all_rows_unq = sorted_group.drop_duplicates(['tw_usr_screen'])\n",
    "    \n",
    "    # Aggregate the `retweet_cnt`, `rtw_usr_screen`, and `tw_usr_screen` columns of all the rows\n",
    "    first_60_percent_agg = {\n",
    "                'rt_url': name,\n",
    "        'retweet_cnt_sum_60': first_60_percent_unq['retweet_cnt'].sum(),\n",
    "        'rtw_usr_screen_list_60': tuple(set(first_60_percent['rtw_usr_screen'].tolist())),\n",
    "        'tw_usr_screen_list_60': tuple(set(first_60_percent['tw_usr_screen'].tolist())),\n",
    "\n",
    "#         'rt_url': name,\n",
    "        'retweet_cnt_sum_all': all_rows_unq['retweet_cnt'].sum(),\n",
    "        'rtw_usr_screen_list_all': tuple(set(all_rows['rtw_usr_screen'].tolist())),\n",
    "        'tw_usr_screen_list_all': tuple(set(all_rows['tw_usr_screen'].tolist()))\n",
    "    }\n",
    "    \n",
    "    # Append the results for this group to the overall list of results\n",
    "    results.append(first_60_percent_agg)\n",
    "#     results.append(all_rows_agg)\n",
    "\n",
    "# Create a new DataFrame from the results\n",
    "result_df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e3c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_download(Pickle_path+\"URL_base_data_train_test.pkl\",result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48dd7745",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = result_df[(result_df['retweet_cnt_sum_60']>1000) & (result_df['retweet_cnt_sum_all']>1000) & (result_df['retweet_cnt_sum_all']!=result_df['retweet_cnt_sum_60'])].\\\n",
    "                sort_values(['retweet_cnt_sum_all'],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68682481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt_url</th>\n",
       "      <th>retweet_cnt_sum_60</th>\n",
       "      <th>rtw_usr_screen_list_60</th>\n",
       "      <th>tw_usr_screen_list_60</th>\n",
       "      <th>retweet_cnt_sum_all</th>\n",
       "      <th>rtw_usr_screen_list_all</th>\n",
       "      <th>tw_usr_screen_list_all</th>\n",
       "      <th>usr_screen_60</th>\n",
       "      <th>usr_screen_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34652</th>\n",
       "      <td>https://support.twitter.com/articles/20169199</td>\n",
       "      <td>123390</td>\n",
       "      <td>(Theboydrool, CongealedM, pietrohiq, TDarkhand...</td>\n",
       "      <td>(joembayawaphoto, ChocolateWraith, _stylingg, ...</td>\n",
       "      <td>728079</td>\n",
       "      <td>(Hero2113805826, fatgrlfantasies, VicSanityX, ...</td>\n",
       "      <td>(ChocolateWraith, NickFittXXX, NYGovCuomo, Mas...</td>\n",
       "      <td>(ChocolateWraith, NickFittXXX, Hero2113805826,...</td>\n",
       "      <td>(ChocolateWraith, NickFittXXX, Hero2113805826,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47731</th>\n",
       "      <td>https://twitter.com/WFLA/status/12598673542065...</td>\n",
       "      <td>97891</td>\n",
       "      <td>(TomLazerBum, karenjiggetts, jordieleigh5, Bik...</td>\n",
       "      <td>(Nick__Pulos, QuincyEnunwa, WhomstSaidThat, gr...</td>\n",
       "      <td>100533</td>\n",
       "      <td>(TomLazerBum, karenjiggetts, JohnCrowell, Des_...</td>\n",
       "      <td>(TedRogersLA, Nick__Pulos, oneunderscore__, ad...</td>\n",
       "      <td>(TomLazerBum, karenjiggetts, Nick__Pulos, jord...</td>\n",
       "      <td>(Des_Shinta, jordieleigh5, oneunderscore__, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41826</th>\n",
       "      <td>https://twitter.com/CNBC/status/12635109230561...</td>\n",
       "      <td>28878</td>\n",
       "      <td>(AndyLeeParker1, DaultRadio, Vanessa64254192, ...</td>\n",
       "      <td>(PatrioticMills, jaredskolnick, Pat_Thorman, k...</td>\n",
       "      <td>72162</td>\n",
       "      <td>(ReginaRed4, IBRustyJim, AndyLeeParker1, jetti...</td>\n",
       "      <td>(PatrioticMills, jaredskolnick, Pat_Thorman, h...</td>\n",
       "      <td>(PatrioticMills, jaredskolnick, AndyLeeParker1...</td>\n",
       "      <td>(jaredskolnick, jettisonbaggage, haymarketbook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52380</th>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/123...</td>\n",
       "      <td>36124</td>\n",
       "      <td>(Miss_EmmaMae, l_dermody, WolfWar72757268, Arm...</td>\n",
       "      <td>(JoeBiden, Leslieoo7, freddyatton, markmobility)</td>\n",
       "      <td>36148</td>\n",
       "      <td>(l_dermody, asher_971, Miss_EmmaMae, ArmandoBr...</td>\n",
       "      <td>(freddyatton, markmobility, AndrewFeinberg, Le...</td>\n",
       "      <td>(l_dermody, freddyatton, markmobility, Leslieo...</td>\n",
       "      <td>(l_dermody, freddyatton, markmobility, AndrewF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53155</th>\n",
       "      <td>https://twitter.com/thehill/status/12599411129...</td>\n",
       "      <td>24933</td>\n",
       "      <td>(Torey_Elwell, bethlastname, Nebby_99)</td>\n",
       "      <td>(mmpadellan, foxxi_loxxi, ewarren)</td>\n",
       "      <td>25121</td>\n",
       "      <td>(Torey_Elwell, besthealthyou, bethlastname, Ne...</td>\n",
       "      <td>(mmpadellan, foxxi_loxxi, Public_Citizen, ewar...</td>\n",
       "      <td>(Torey_Elwell, foxxi_loxxi, Nebby_99, ewarren,...</td>\n",
       "      <td>(Public_Citizen, Torey_Elwell, foxxi_loxxi, Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48140</th>\n",
       "      <td>https://twitter.com/ajc/status/125624854552037...</td>\n",
       "      <td>1014</td>\n",
       "      <td>(GeecheeGirlCafe, shayne571, NewAugustWest, Ma...</td>\n",
       "      <td>(theferocity, AngryBlackLady, Wilson__Valdez, ...</td>\n",
       "      <td>1069</td>\n",
       "      <td>(GeecheeGirlCafe, shayne571, NewAugustWest, Ma...</td>\n",
       "      <td>(ValerieValHalla, theferocity, rtajima, AngryB...</td>\n",
       "      <td>(theferocity, GeecheeGirlCafe, shayne571, NewA...</td>\n",
       "      <td>(shayne571, rtajima, AngryBlackLady, Wilson__V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80762</th>\n",
       "      <td>https://www.vanityfair.com/news/2020/05/trump-...</td>\n",
       "      <td>1027</td>\n",
       "      <td>(BenjaminBaughm2, FGedrich, IDWoods, kernelken2)</td>\n",
       "      <td>(TeaPainUSA, BobKustra, LaurieHosken, leonberg...</td>\n",
       "      <td>1047</td>\n",
       "      <td>(IDWoods, waterloosunset, BenjaminBaughm2, FGe...</td>\n",
       "      <td>(LaurieHosken, TeaPainUSA, BobKustra, Dr_Shara...</td>\n",
       "      <td>(IDWoods, LaurieHosken, BenjaminBaughm2, FGedr...</td>\n",
       "      <td>(IDWoods, LaurieHosken, waterloosunset, Benjam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45396</th>\n",
       "      <td>https://twitter.com/NatashaBertrand/status/126...</td>\n",
       "      <td>1042</td>\n",
       "      <td>(cgormley9, ohiovoter55)</td>\n",
       "      <td>(brianbeutler, glennkirschner2)</td>\n",
       "      <td>1043</td>\n",
       "      <td>(KeithTatem, cgormley9, ohiovoter55)</td>\n",
       "      <td>(brianbeutler, SandyGirl4Him, glennkirschner2)</td>\n",
       "      <td>(brianbeutler, cgormley9, glennkirschner2, ohi...</td>\n",
       "      <td>(ohiovoter55, KeithTatem, cgormley9, glennkirs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52313</th>\n",
       "      <td>https://twitter.com/puckettwrites/status/12595...</td>\n",
       "      <td>1014</td>\n",
       "      <td>(park_congress, SLKath, GangstaOma)</td>\n",
       "      <td>(TayAndersonCO, kathyredmond, mmpadellan)</td>\n",
       "      <td>1039</td>\n",
       "      <td>(park_congress, SLKath, GangstaOma, QueerjohnPA)</td>\n",
       "      <td>(TayAndersonCO, kathyredmond, NC5PhilWilliams,...</td>\n",
       "      <td>(SLKath, GangstaOma, kathyredmond, TayAnderson...</td>\n",
       "      <td>(SLKath, GangstaOma, kathyredmond, TayAnderson...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81820</th>\n",
       "      <td>https://www.washingtonpost.com/politics/trump-...</td>\n",
       "      <td>1018</td>\n",
       "      <td>(solomonmissouri, DebraDl1288us, AmyRaines8, Z...</td>\n",
       "      <td>(JonathanMetzl, JoeNBC, BranGoch, karolcummins)</td>\n",
       "      <td>1023</td>\n",
       "      <td>(jimmytheplant, GiGi_1939, DebraDl1288us, AmyR...</td>\n",
       "      <td>(DanelleK, BranGoch, karolcummins, JonathanMet...</td>\n",
       "      <td>(karolcummins, DebraDl1288us, AmyRaines8, Zomb...</td>\n",
       "      <td>(jimmytheplant, DanelleK, GiGi_1939, karolcumm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  rt_url  retweet_cnt_sum_60  \\\n",
       "34652      https://support.twitter.com/articles/20169199              123390   \n",
       "47731  https://twitter.com/WFLA/status/12598673542065...               97891   \n",
       "41826  https://twitter.com/CNBC/status/12635109230561...               28878   \n",
       "52380  https://twitter.com/realDonaldTrump/status/123...               36124   \n",
       "53155  https://twitter.com/thehill/status/12599411129...               24933   \n",
       "...                                                  ...                 ...   \n",
       "48140  https://twitter.com/ajc/status/125624854552037...                1014   \n",
       "80762  https://www.vanityfair.com/news/2020/05/trump-...                1027   \n",
       "45396  https://twitter.com/NatashaBertrand/status/126...                1042   \n",
       "52313  https://twitter.com/puckettwrites/status/12595...                1014   \n",
       "81820  https://www.washingtonpost.com/politics/trump-...                1018   \n",
       "\n",
       "                                  rtw_usr_screen_list_60  \\\n",
       "34652  (Theboydrool, CongealedM, pietrohiq, TDarkhand...   \n",
       "47731  (TomLazerBum, karenjiggetts, jordieleigh5, Bik...   \n",
       "41826  (AndyLeeParker1, DaultRadio, Vanessa64254192, ...   \n",
       "52380  (Miss_EmmaMae, l_dermody, WolfWar72757268, Arm...   \n",
       "53155             (Torey_Elwell, bethlastname, Nebby_99)   \n",
       "...                                                  ...   \n",
       "48140  (GeecheeGirlCafe, shayne571, NewAugustWest, Ma...   \n",
       "80762   (BenjaminBaughm2, FGedrich, IDWoods, kernelken2)   \n",
       "45396                           (cgormley9, ohiovoter55)   \n",
       "52313                (park_congress, SLKath, GangstaOma)   \n",
       "81820  (solomonmissouri, DebraDl1288us, AmyRaines8, Z...   \n",
       "\n",
       "                                   tw_usr_screen_list_60  retweet_cnt_sum_all  \\\n",
       "34652  (joembayawaphoto, ChocolateWraith, _stylingg, ...               728079   \n",
       "47731  (Nick__Pulos, QuincyEnunwa, WhomstSaidThat, gr...               100533   \n",
       "41826  (PatrioticMills, jaredskolnick, Pat_Thorman, k...                72162   \n",
       "52380   (JoeBiden, Leslieoo7, freddyatton, markmobility)                36148   \n",
       "53155                 (mmpadellan, foxxi_loxxi, ewarren)                25121   \n",
       "...                                                  ...                  ...   \n",
       "48140  (theferocity, AngryBlackLady, Wilson__Valdez, ...                 1069   \n",
       "80762  (TeaPainUSA, BobKustra, LaurieHosken, leonberg...                 1047   \n",
       "45396                    (brianbeutler, glennkirschner2)                 1043   \n",
       "52313          (TayAndersonCO, kathyredmond, mmpadellan)                 1039   \n",
       "81820    (JonathanMetzl, JoeNBC, BranGoch, karolcummins)                 1023   \n",
       "\n",
       "                                 rtw_usr_screen_list_all  \\\n",
       "34652  (Hero2113805826, fatgrlfantasies, VicSanityX, ...   \n",
       "47731  (TomLazerBum, karenjiggetts, JohnCrowell, Des_...   \n",
       "41826  (ReginaRed4, IBRustyJim, AndyLeeParker1, jetti...   \n",
       "52380  (l_dermody, asher_971, Miss_EmmaMae, ArmandoBr...   \n",
       "53155  (Torey_Elwell, besthealthyou, bethlastname, Ne...   \n",
       "...                                                  ...   \n",
       "48140  (GeecheeGirlCafe, shayne571, NewAugustWest, Ma...   \n",
       "80762  (IDWoods, waterloosunset, BenjaminBaughm2, FGe...   \n",
       "45396               (KeithTatem, cgormley9, ohiovoter55)   \n",
       "52313   (park_congress, SLKath, GangstaOma, QueerjohnPA)   \n",
       "81820  (jimmytheplant, GiGi_1939, DebraDl1288us, AmyR...   \n",
       "\n",
       "                                  tw_usr_screen_list_all  \\\n",
       "34652  (ChocolateWraith, NickFittXXX, NYGovCuomo, Mas...   \n",
       "47731  (TedRogersLA, Nick__Pulos, oneunderscore__, ad...   \n",
       "41826  (PatrioticMills, jaredskolnick, Pat_Thorman, h...   \n",
       "52380  (freddyatton, markmobility, AndrewFeinberg, Le...   \n",
       "53155  (mmpadellan, foxxi_loxxi, Public_Citizen, ewar...   \n",
       "...                                                  ...   \n",
       "48140  (ValerieValHalla, theferocity, rtajima, AngryB...   \n",
       "80762  (LaurieHosken, TeaPainUSA, BobKustra, Dr_Shara...   \n",
       "45396     (brianbeutler, SandyGirl4Him, glennkirschner2)   \n",
       "52313  (TayAndersonCO, kathyredmond, NC5PhilWilliams,...   \n",
       "81820  (DanelleK, BranGoch, karolcummins, JonathanMet...   \n",
       "\n",
       "                                           usr_screen_60  \\\n",
       "34652  (ChocolateWraith, NickFittXXX, Hero2113805826,...   \n",
       "47731  (TomLazerBum, karenjiggetts, Nick__Pulos, jord...   \n",
       "41826  (PatrioticMills, jaredskolnick, AndyLeeParker1...   \n",
       "52380  (l_dermody, freddyatton, markmobility, Leslieo...   \n",
       "53155  (Torey_Elwell, foxxi_loxxi, Nebby_99, ewarren,...   \n",
       "...                                                  ...   \n",
       "48140  (theferocity, GeecheeGirlCafe, shayne571, NewA...   \n",
       "80762  (IDWoods, LaurieHosken, BenjaminBaughm2, FGedr...   \n",
       "45396  (brianbeutler, cgormley9, glennkirschner2, ohi...   \n",
       "52313  (SLKath, GangstaOma, kathyredmond, TayAnderson...   \n",
       "81820  (karolcummins, DebraDl1288us, AmyRaines8, Zomb...   \n",
       "\n",
       "                                          usr_screen_all  \n",
       "34652  (ChocolateWraith, NickFittXXX, Hero2113805826,...  \n",
       "47731  (Des_Shinta, jordieleigh5, oneunderscore__, am...  \n",
       "41826  (jaredskolnick, jettisonbaggage, haymarketbook...  \n",
       "52380  (l_dermody, freddyatton, markmobility, AndrewF...  \n",
       "53155  (Public_Citizen, Torey_Elwell, foxxi_loxxi, Ne...  \n",
       "...                                                  ...  \n",
       "48140  (shayne571, rtajima, AngryBlackLady, Wilson__V...  \n",
       "80762  (IDWoods, LaurieHosken, waterloosunset, Benjam...  \n",
       "45396  (ohiovoter55, KeithTatem, cgormley9, glennkirs...  \n",
       "52313  (SLKath, GangstaOma, kathyredmond, TayAnderson...  \n",
       "81820  (jimmytheplant, DanelleK, GiGi_1939, karolcumm...  \n",
       "\n",
       "[208 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "793f47c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_download(Pickle_path+\"URL_base_data_train_test_POC.pkl\",sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087eb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['usr_screen_60'] = sample_data.apply(lambda row: tuple(set((row['rtw_usr_screen_list_60']+ row['tw_usr_screen_list_60']))), axis=1)\n",
    "\n",
    "\n",
    "sample_data['usr_screen_all'] = sample_data.apply(lambda row: tuple(set((row['rtw_usr_screen_list_all']+ row['tw_usr_screen_list_all']))), axis=1)\n",
    "\n",
    "\n",
    "def extract_user_ids(screen_names, mapping_dict):\n",
    "    user_ids = []\n",
    "    for screen_name in screen_names:\n",
    "        if screen_name in mapping_dict:\n",
    "            user_ids.append(mapping_dict[screen_name])\n",
    "    return tuple(user_ids)\n",
    "\n",
    "\n",
    "sample_data['train_user_ids'] = sample_data['usr_screen_60'].apply(lambda x: extract_user_ids( x, tw_usr_screen_dict))\n",
    "sample_data['test_user_ids'] = sample_data['usr_screen_all'].apply(lambda x: extract_user_ids(x, tw_usr_screen_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "60adc652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rt_url</th>\n",
       "      <th>retweet_cnt_sum_60</th>\n",
       "      <th>rtw_usr_screen_list_60</th>\n",
       "      <th>tw_usr_screen_list_60</th>\n",
       "      <th>retweet_cnt_sum_all</th>\n",
       "      <th>rtw_usr_screen_list_all</th>\n",
       "      <th>tw_usr_screen_list_all</th>\n",
       "      <th>usr_screen_60</th>\n",
       "      <th>usr_screen_all</th>\n",
       "      <th>train_user_ids</th>\n",
       "      <th>test_user_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34652</th>\n",
       "      <td>https://support.twitter.com/articles/20169199</td>\n",
       "      <td>123390</td>\n",
       "      <td>(Treycarey8, Jboogie16705348, 2020Dennison, dv...</td>\n",
       "      <td>(RondaRousey, TrayTheTrade, NBCNews, DaddyMrCa...</td>\n",
       "      <td>728079</td>\n",
       "      <td>(Jboogie16705348, dvlzura, cruzz_bi, turboanda...</td>\n",
       "      <td>(RondaRousey, SenTedCruz, _stylingg, PeepshowM...</td>\n",
       "      <td>(RondaRousey, Jboogie16705348, dvlzura, SenTed...</td>\n",
       "      <td>(RondaRousey, Jboogie16705348, dvlzura, SenTed...</td>\n",
       "      <td>(82588, 62644, 35605, 8741, 25542, 98694, 3735...</td>\n",
       "      <td>(82588, 62644, 35605, 8741, 25542, 14824, 3083...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47731</th>\n",
       "      <td>https://twitter.com/WFLA/status/12598673542065...</td>\n",
       "      <td>97891</td>\n",
       "      <td>(jordieleigh5, ItsMyOpinion16, TomLazerBum, St...</td>\n",
       "      <td>(mvzelenks, WhomstSaidThat, GoldnSweetCheek, A...</td>\n",
       "      <td>100533</td>\n",
       "      <td>(scraw28, jordieleigh5, ItsMyOpinion16, TomLaz...</td>\n",
       "      <td>(mvzelenks, WhomstSaidThat, BrianPShea, GoldnS...</td>\n",
       "      <td>(mvzelenks, WhomstSaidThat, jordieleigh5, Gold...</td>\n",
       "      <td>(NayrmanBSC, StaceyExplosion, JeffJSays, Nick_...</td>\n",
       "      <td>(53445, 18997, 47250, 58689, 66875, 87987, 801...</td>\n",
       "      <td>(11090, 80158, 62345, 98662, 35126, 25409, 540...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  rt_url  retweet_cnt_sum_60  \\\n",
       "34652      https://support.twitter.com/articles/20169199              123390   \n",
       "47731  https://twitter.com/WFLA/status/12598673542065...               97891   \n",
       "\n",
       "                                  rtw_usr_screen_list_60  \\\n",
       "34652  (Treycarey8, Jboogie16705348, 2020Dennison, dv...   \n",
       "47731  (jordieleigh5, ItsMyOpinion16, TomLazerBum, St...   \n",
       "\n",
       "                                   tw_usr_screen_list_60  retweet_cnt_sum_all  \\\n",
       "34652  (RondaRousey, TrayTheTrade, NBCNews, DaddyMrCa...               728079   \n",
       "47731  (mvzelenks, WhomstSaidThat, GoldnSweetCheek, A...               100533   \n",
       "\n",
       "                                 rtw_usr_screen_list_all  \\\n",
       "34652  (Jboogie16705348, dvlzura, cruzz_bi, turboanda...   \n",
       "47731  (scraw28, jordieleigh5, ItsMyOpinion16, TomLaz...   \n",
       "\n",
       "                                  tw_usr_screen_list_all  \\\n",
       "34652  (RondaRousey, SenTedCruz, _stylingg, PeepshowM...   \n",
       "47731  (mvzelenks, WhomstSaidThat, BrianPShea, GoldnS...   \n",
       "\n",
       "                                           usr_screen_60  \\\n",
       "34652  (RondaRousey, Jboogie16705348, dvlzura, SenTed...   \n",
       "47731  (mvzelenks, WhomstSaidThat, jordieleigh5, Gold...   \n",
       "\n",
       "                                          usr_screen_all  \\\n",
       "34652  (RondaRousey, Jboogie16705348, dvlzura, SenTed...   \n",
       "47731  (NayrmanBSC, StaceyExplosion, JeffJSays, Nick_...   \n",
       "\n",
       "                                          train_user_ids  \\\n",
       "34652  (82588, 62644, 35605, 8741, 25542, 98694, 3735...   \n",
       "47731  (53445, 18997, 47250, 58689, 66875, 87987, 801...   \n",
       "\n",
       "                                           test_user_ids  \n",
       "34652  (82588, 62644, 35605, 8741, 25542, 14824, 3083...  \n",
       "47731  (11090, 80158, 62345, 98662, 35126, 25409, 540...  "
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da12932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor each URL before 21 May 2020, identify when it was first tweeted, put cut off of 1 week and find all ids which tweeted that url within that timeframe\\n\\nBut for Y ( retweet count): Use the retweet count 3 weeks after first occurance of tweet\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Approach 2\n",
    "'''\n",
    "For each URL before 21 May 2020, identify when it was first tweeted, put cut off of 1 week and find all ids which tweeted that url within that timeframe\n",
    "\n",
    "But for Y ( retweet count): Use the retweet count 3 weeks after first occurance of tweet\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2a1256f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5706cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a41812d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = list(master_df['rt_url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "91a98986",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data_dict = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eac84586",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_url in url_list[:]:\n",
    "#     input_url = 'https://support.twitter.com/articles/20169199'\n",
    "    test_urldf = master_df[master_df['rt_url']==input_url]\n",
    "\n",
    "    retwet_cnt = test_urldf[['tw_usr_screen','retweet_cnt']].drop_duplicates(['tw_usr_screen'])['retweet_cnt'].sum()\n",
    "    if retwet_cnt<1000:\n",
    "        continue\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "\n",
    "    # Create sample DataFrame\n",
    "    # Define original and new formats\n",
    "    original_format = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "    new_format = \"%Y-%m-%d %H:%M:%S%z\"\n",
    "\n",
    "    # Define conversion function\n",
    "    def convert_time(original_string):\n",
    "        dt = datetime.strptime(original_string, original_format).replace(tzinfo=pytz.UTC)\n",
    "        return dt.strftime(new_format)\n",
    "\n",
    "    # Apply conversion function to column\n",
    "\n",
    "\n",
    "    retweetdf_tmp= test_urldf[['rtw_time','rtw_usr_screen']]\n",
    "    retweetdf_tmp.columns = ['tw_time','tw_usr_screen']\n",
    "    retweetdf_tmp['tw_time'] = retweetdf_tmp['tw_time'].apply(lambda x: convert_time(x))\n",
    "    original_format = '%Y-%m-%d'\n",
    "\n",
    "    # Convert column to datetime objects\n",
    "    retweetdf_tmp['tw_time'] = retweetdf_tmp['tw_time'].apply(lambda x: datetime.strptime(x[:10], original_format).date())\n",
    "\n",
    "\n",
    "\n",
    "    tweet_urldf_1 = test_urldf[['tw_time','tw_usr_screen']]\n",
    "    tweet_urldf_1['tw_time'] = tweet_urldf_1['tw_time'].apply(lambda x : x.date())\n",
    "\n",
    "    url_all_users = pd.concat([retweetdf_tmp,\\\n",
    "              tweet_urldf_1],axis=0).drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "    min_time = url_all_users['tw_time'].min()\n",
    "\n",
    "    import datetime\n",
    "\n",
    "    min_time = url_all_users['tw_time'].min()\n",
    "\n",
    "    if min_time < datetime.date(2023, 5, 16):\n",
    "        one_week = datetime.timedelta(weeks=1)\n",
    "        one_week_after_min = min_time + one_week\n",
    "        filtered_df = url_all_users[url_all_users['tw_time'] <= one_week_after_min]\n",
    "#         print(len(filtered_df))\n",
    "        if len(filtered_df)<2:\n",
    "            continue\n",
    "        Train_data_dict[input_url] = {\"Users\": tuple(set(filtered_df['tw_usr_screen'].tolist())), \"Retweet_cnt\": retwet_cnt}\n",
    "    if min_time > datetime.date(2023, 5, 21):\n",
    "        one_week = datetime.timedelta(weeks=1)\n",
    "        one_week_after_min = min_time + one_week\n",
    "        filtered_df = url_all_users[url_all_users['tw_time'] <= one_week_after_min]\n",
    "#         print(len(filtered_df))\n",
    "        if len(filtered_df)<2:\n",
    "            continue\n",
    "        Test_data_dict[input_url] = {\"Users\": tuple(set(filtered_df['tw_usr_screen'].tolist())), \"Retweet_cnt\": retwet_cnt}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "96c10cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_download(Pickle_path+\"Train_data_dict_1_week_tmp.pkl\",Train_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ff09ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "User_level_feat_df = pd.read_csv(csv_path+\"User_profile.csv\")\n",
    "\n",
    "# assume your DataFrame is named df\n",
    "tw_usr_screen_dict = User_level_feat_df.reset_index().set_index('tw_usr_screen')['index'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f3314398",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data_dict_new = {}\n",
    "for key,val in Train_data_dict.items():\n",
    "    new_vals = val['Users']\n",
    "    val['Users'] = extract_user_ids( new_vals, tw_usr_screen_dict)\n",
    "    Train_data_dict_new[key] ={'Users':extract_user_ids( new_vals, tw_usr_screen_dict),\"Retweet_cnt\":val['Retweet_cnt']}\n",
    "    \n",
    "\n",
    "# sample_data['train_user_ids'] = sample_data['usr_screen_60'].apply(lambda x: extract_user_ids( x, tw_usr_screen_dict))\n",
    "# sample_data['test_user_ids'] = sample_data['usr_screen_all'].apply(lambda x: extract_user_ids(x, tw_usr_screen_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5b2f5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_download(Pickle_path+\"Train_data_dict_1_week.pkl\",Train_data_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4d10dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_download(Pickle_path+\"Test_data_dict_1_week.pkl\",Test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "53302111",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_subset = list(Train_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "1a86b05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_data = master_df[master_df['rt_url'].isin(url_subset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "491521ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dict_idx = {}\n",
    "\n",
    "for user,adj in similarity_dict.items():\n",
    "    inner_dict = {}\n",
    "    for key,val in adj.items():\n",
    "        inner_dict[tw_usr_screen_dict[key]] = val\n",
    "    similarity_dict_idx[tw_usr_screen_dict[user]] = inner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "560aa4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "Follower_cnt_dict_idx = {}\n",
    "\n",
    "for key, val in Follower_cnt_dict.items():\n",
    "    Follower_cnt_dict_idx[tw_usr_screen_dict[key]] = val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
